{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL IMPORTS\n",
    "\n",
    "import torch\n",
    "from mapping.datasets import RLDatasetFormatter\n",
    "from mapping.models import Discriminator, TransitionModel\n",
    "from mapping.models.autoencoder_TD import LitAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = 'pend2mc_TD_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING OF DATASETS\n",
    "\n",
    "s_s_max = torch.tensor([1.0, 1.0, 8.0])\n",
    "s_s_min = torch.tensor([-1.0, -1.0, -8.0])\n",
    "s_a_max = torch.tensor([2.0])\n",
    "s_a_min = torch.tensor([-2.0])\n",
    "\n",
    "t_s_max = torch.tensor([0.6, 0.07])\n",
    "t_s_min = torch.tensor([-1.2, -0.07])\n",
    "t_a_max = torch.tensor([1.0])\n",
    "t_a_min = torch.tensor([-1.0])\n",
    "\n",
    "dataset_t_path = 'data/UntrainedMCDataset500.csv'\n",
    "dataset_s_path = 'data/UntrainedPendDataset5000.csv'\n",
    "dataset_s_path_2 = 'data/UntrainedPendDataset2500.csv'\n",
    "dataset_t_path_ext = 'data/UntrainedMCDataset5000_2.csv'\n",
    "\n",
    "data_formatter_t = RLDatasetFormatter().from_csv(dataset_t_path).normalize_data(t_s_max, t_s_min, t_a_max, t_a_min)\n",
    "data_formatter_s = RLDatasetFormatter().from_csv(dataset_s_path).normalize_data(s_s_max, s_s_min, s_a_max, s_a_min)\n",
    "data_formatter_s_2 = RLDatasetFormatter().from_csv(dataset_s_path_2).normalize_data(s_s_max, s_s_min, s_a_max, s_a_min)\n",
    "data_formatter_t_ext = RLDatasetFormatter().from_csv(dataset_t_path_ext).normalize_data(t_s_max, t_s_min, t_a_max, t_a_min)\n",
    "\n",
    "s_s_size = data_formatter_s.state_size\n",
    "s_a_size = data_formatter_s.action_size\n",
    "t_s_size = data_formatter_t.state_size\n",
    "t_a_size = data_formatter_t.action_size \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_lr = 0.001\n",
    "T_epochs = 100\n",
    "T_batch_size = 100\n",
    "T = TransitionModel(lr=T_lr,\n",
    "                    s_dim=t_s_size,\n",
    "                    a_dim=t_a_size)\n",
    "T.train_model(dataset=data_formatter_t.as_transitions(),\n",
    "              batch_size=T_batch_size,\n",
    "              epochs=T_epochs,\n",
    "              logs_dir=LOG_DIR+'/T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING OF DISCRIMINATOR\n",
    "D_lr = 0.0001\n",
    "D_epochs = 100\n",
    "D_batch_size = 100\n",
    "\n",
    "# def generate_fake_dataset(shape, s_size, a_size):\n",
    "#     data = torch.rand(shape[0], shape[1]+1) # +1 Ã¨ per il reward\n",
    "#     fake_s, fake_a, fake_r, fake_s1 = torch.split(data, [s_size, a_size, 1, s_size],1)\n",
    "#     in_fake_dataset = RLDatasetFormatter([fake_s, fake_a, fake_r, fake_s1]).transition_as_fake()\n",
    "\n",
    "\n",
    "\n",
    "disc_dataset = data_formatter_t.transition_as_valid()\n",
    "\n",
    "D = Discriminator(lr=D_lr, s_dim=t_s_size, a_dim=t_a_size)\n",
    "D.train_model(disc_dataset, logs_dir=LOG_DIR+'/Discriminator', epochs=D_epochs, batch_size=D_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_lr = 0.001\n",
    "AE_epochs = 50\n",
    "AE_batch_size = 50\n",
    "AE_lambdas = (1,1,5)\n",
    "D_lr = 0.001\n",
    "D_epochs = 20\n",
    "D_batch_size = 50\n",
    "ADV_ITERATIONS = 20\n",
    "\n",
    "AE = LitAutoEncoder(lr=AE_lr,\n",
    "                    s_s_size=s_s_size, \n",
    "                    s_a_size=s_a_size, \n",
    "                    t_s_size=t_s_size, \n",
    "                    t_a_size=t_a_size,\n",
    "                    D=D.as_dict()['func'],\n",
    "                    T=T.as_dict()['func'],\n",
    "                    lambdas=AE_lambdas)\n",
    "\n",
    "for i in range(ADV_ITERATIONS):\n",
    "    AE.D = D.as_dict()['func']\n",
    "\n",
    "    AE.train_model(dataset=data_formatter_s.transition_identity(),\n",
    "                batch_size=AE_batch_size,\n",
    "                epochs=AE_epochs,\n",
    "                logs_dir=LOG_DIR+'/AE')\n",
    "\n",
    "    M = AE.as_dict()[\"M\"]\n",
    "    test_dataset_s = data_formatter_s_2.transition_identity()\n",
    "    test_dataset_s.shuffle()\n",
    "    sas, _ = test_dataset_s[: len(data_formatter_t.as_transitions())]\n",
    "    with torch.no_grad():\n",
    "        synthetic_data = M(sas)\n",
    "\n",
    "    r_synth = torch.zeros((synthetic_data.shape[0], 1))\n",
    "    s_synth, a_synth, s1_synth = torch.split(synthetic_data,[t_s_size,t_a_size,t_s_size],1)\n",
    "    data_formatter_synth = RLDatasetFormatter([s_synth, a_synth, r_synth, s1_synth])\n",
    "\n",
    "    disc_dataset = data_formatter_t.transition_as_valid()\n",
    "    fake_samples = data_formatter_synth.transition_as_fake()\n",
    "    disc_dataset = disc_dataset.merge(fake_samples)\n",
    "\n",
    "    D.train_model(\n",
    "        disc_dataset,\n",
    "        logs_dir=LOG_DIR+\"/Discriminator\",\n",
    "        batch_size=D_batch_size,\n",
    "        epochs=D_epochs,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_s_2 = data_formatter_s_2.transition_identity()\n",
    "\n",
    "AE_dict = AE.as_dict()\n",
    "\n",
    "sas, _ = dataset_s_2[:]\n",
    "with torch.no_grad():\n",
    "    reconstructed_data = AE_dict['func'](sas)\n",
    "    encoded_data = AE_dict['M'](sas)\n",
    "    \n",
    "#r_synth = torch.zeros((reconstructed_data.shape[0],1))\n",
    "s_rec,a_rec,s1_rec = torch.split(reconstructed_data, [s_s_size, s_a_size, s_s_size],1)\n",
    "s_t_code, a_t_code, s1_t_code = torch.split(encoded_data, [t_s_size, t_a_size, t_s_size],1)\n",
    "#data_formatter_synth = RLDatasetFormatter([s_synth, a_synth, r_synth, s1_synth])\n",
    "s, a, s1 = torch.split(sas, [s_s_size, s_a_size, s_s_size],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2*s_s_size+s_a_size, 2)\n",
    "\n",
    "fig.suptitle('Source Samples vs Reconstructed Samples', fontsize=16)\n",
    "\n",
    "for i in range(s_s_size):\n",
    "    index_data = i\n",
    "    axs[i, 0].plot(s[:,i].detach().numpy())\n",
    "    axs[i, 0].set_title('s['+str(i)+\"]\")\n",
    "    axs[i, 0].grid()\n",
    "    axs[i, 1].plot(s_rec[:,i].detach().numpy())\n",
    "    axs[i, 1].set_title('s_rec['+str(i)+\"]\")\n",
    "    axs[i, 1].grid()\n",
    "for i in range(s_s_size,s_a_size+s_s_size):\n",
    "    index_data = i-s_s_size \n",
    "    axs[i, 0].plot(a[:,index_data].detach().numpy())\n",
    "    axs[i, 0].set_title('a['+str(index_data)+\"]\")\n",
    "    axs[i, 0].grid()\n",
    "    axs[i, 1].plot(a_rec[:,index_data].detach().numpy())\n",
    "    axs[i, 1].set_title('a_rec['+str(index_data)+\"]\")\n",
    "    axs[i, 1].grid\n",
    "for i in range(s_a_size+s_s_size,s_a_size+2*s_s_size):\n",
    "    index_data = i-s_s_size-s_a_size\n",
    "    axs[i, 0].plot(s1[:,index_data].detach().numpy())\n",
    "    axs[i, 0].set_title('s1['+str(index_data)+\"]\")\n",
    "    axs[i, 0].grid()\n",
    "    axs[i, 1].plot(s1_rec[:,index_data].detach().numpy())\n",
    "    axs[i, 1].set_title('s1_rec['+str(index_data)+\"]\")\n",
    "    axs[i, 1].grid()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2*s_s_size+s_a_size, 1)\n",
    "\n",
    "err_s = s-s_rec\n",
    "err_a = a-a_rec\n",
    "err_s1 = s1-s1_rec\n",
    "\n",
    "fig.suptitle('Reconstruction Error per state/action dimension', fontsize=16)\n",
    "\n",
    "for i in range(s_s_size):\n",
    "    axs[i].hist(err_s[:,i].detach().numpy(), bins=1000)\n",
    "    axs[i].set_title('s['+str(i)+\"]-s_rec[\"+str(i)+\"]\")\n",
    "    axs[i].grid()\n",
    "for i in range(s_s_size,s_a_size+s_s_size):\n",
    "    index_data = i-s_s_size \n",
    "    axs[i].hist(err_a[:,index_data].detach().numpy(), bins=1000)\n",
    "    axs[i].set_title('a['+str(index_data)+']-a_rec['+str(index_data)+\"]\")\n",
    "    axs[i].grid()\n",
    "for i in range(s_a_size+s_s_size,s_a_size+2*s_s_size):\n",
    "    index_data = i-s_s_size-s_a_size\n",
    "    axs[i].hist(err_s1[:,index_data].detach().numpy(), bins=1000)\n",
    "    axs[i].set_title('s1['+str(index_data)+\"]-s_rec[\"+str(index_data)+\"]\")\n",
    "    axs[i].grid()\n",
    "fig.set_size_inches(18.5, 18.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2*t_s_size+t_a_size, 1)\n",
    "\n",
    "fig.suptitle('Normalized Generated Samples Distribution', fontsize=16)\n",
    "\n",
    "for i in range(t_s_size):\n",
    "    axs[i].hist(s_t_code[:,i].detach().numpy(), bins=100)\n",
    "    axs[i].set_title('s_t_code['+str(i)+\"]\")\n",
    "    axs[i].grid()\n",
    "for i in range(t_s_size,t_a_size+t_s_size):\n",
    "    index_data = i-t_s_size \n",
    "    axs[i].hist(a_t_code[:,index_data].detach().numpy(), bins=100)\n",
    "    axs[i].set_title('a_t_code['+str(index_data)+']')\n",
    "    axs[i].grid()\n",
    "for i in range(t_a_size+t_s_size,t_a_size+2*t_s_size):\n",
    "    index_data = i-t_s_size-t_a_size\n",
    "    axs[i].hist(s1_t_code[:,index_data].detach().numpy(), bins=100)\n",
    "    axs[i].set_title('s1_t_code['+str(index_data)+\"]\")\n",
    "    axs[i].grid()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_code=torch.zeros(len(s1_t_code),1)\n",
    "synth_data_formatter = RLDatasetFormatter([s_t_code,a_t_code,r_code,s1_t_code]).denormalize_data(t_s_max, t_s_min, t_a_max, t_a_min)\n",
    "\n",
    "s_t_code, a_t_code, s1_t_code = torch.split(synth_data_formatter.transition_identity()[:][0], [t_s_size, t_a_size, t_s_size],1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2*t_s_size+t_a_size, 1)\n",
    "\n",
    "fig.suptitle('Generated Samples Distribution', fontsize=16)\n",
    "\n",
    "for i in range(t_s_size):\n",
    "    axs[i].hist(s_t_code[:,i].detach().numpy(), bins=100)\n",
    "    axs[i].set_title('s_t_code['+str(i)+\"]\")\n",
    "    axs[i].grid()\n",
    "for i in range(t_s_size,t_a_size+t_s_size):\n",
    "    index_data = i-t_s_size \n",
    "    axs[i].hist(a_t_code[:,index_data].detach().numpy(), bins=100)\n",
    "    axs[i].set_title('a_t_code['+str(index_data)+']')\n",
    "    axs[i].grid()\n",
    "for i in range(t_a_size+t_s_size,t_a_size+2*t_s_size):\n",
    "    index_data = i-t_s_size-t_a_size\n",
    "    axs[i].hist(s1_t_code[:,index_data].detach().numpy(), bins=100)\n",
    "    axs[i].set_title('s1_t_code['+str(index_data)+\"]\")\n",
    "    axs[i].grid()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_dataset_t = synth_data_formatter.normalize_data(t_s_max, t_s_min, t_a_max, t_a_min).as_transitions()\n",
    "dataset_t = data_formatter_t.as_transitions()\n",
    "train_dataset=synth_dataset_t.merge(dataset_t)\n",
    "\n",
    "T_hat = TransitionModel(lr=T_lr,\n",
    "                        s_dim=t_s_size,\n",
    "                        a_dim=t_a_size)\n",
    "T_hat.train_model(dataset=train_dataset,\n",
    "              batch_size=T_batch_size,\n",
    "              epochs=T_epochs,\n",
    "              logs_dir=LOG_DIR+'/T_hat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_t_ext = data_formatter_t_ext.as_transitions()\n",
    "\n",
    "sa, s1 = dataset_t_ext[:]\n",
    "T_fun = T.as_dict()['func']\n",
    "T_hat_fun = T_hat.as_dict()['func']\n",
    "with torch.no_grad():\n",
    "    s1_T = T_fun(sa)\n",
    "    s1_hat_T = T_hat_fun(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(t_s_size, 1)\n",
    "\n",
    "\n",
    "fig.suptitle('Normalized Prediction Error per state dimension', fontsize=16)\n",
    "\n",
    "err_T = s1-s1_T\n",
    "err_hat_T = s1-s1_hat_T\n",
    "\n",
    "for i in range(t_s_size):\n",
    "    axs[i].hist(err_T[:,i].detach().numpy(), bins=1000, label=\"T\")\n",
    "    axs[i].hist(err_hat_T[:,i].detach().numpy(), bins=1000, label=\"T_hat\")\n",
    "    axs[i].set_title('s1['+str(i)+\"]-s1_T[\"+str(i)+\"]\")\n",
    "    axs[i].grid()\n",
    "    axs[i].legend()\n",
    "\n",
    "fig.set_size_inches(18.5, 14.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (18.5, 10.5)\n",
    "\n",
    "fig.suptitle('Normalized Mean Squared Error per state (sorted)', fontsize=16)\n",
    "err_T = torch.norm(s1-s1_T, dim=1)\n",
    "err_hat_T = torch.norm(s1-s1_hat_T, dim=1)\n",
    "\n",
    "plt.plot(sorted(err_T.detach().numpy(),reverse=True), label='T')\n",
    "plt.plot(sorted(err_hat_T.detach().numpy(),reverse=True), label=\"T_hat\")\n",
    "plt.title('s1['+str(i)+\"]-s1_T[\"+str(i)+\"]\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (18.5, 10.5)\n",
    "\n",
    "plt.title('Mean Squared Error per state (sorted)', fontsize=16)\n",
    "den_s1_T = s1_T * (t_s_max - t_s_min) + t_s_min\n",
    "den_s1_hat_T = s1_hat_T * (t_s_max - t_s_min) + t_s_min\n",
    "den_s1 = s1 * (t_s_max - t_s_min) + t_s_min\n",
    "err_T = torch.norm(den_s1-den_s1_T, dim=1)\n",
    "err_hat_T = torch.norm(den_s1-den_s1_hat_T, dim=1)\n",
    "\n",
    "plt.plot(sorted(err_T.detach().numpy(),reverse=True), label='T')\n",
    "plt.plot(sorted(err_hat_T.detach().numpy(),reverse=True), label=\"T_hat\")\n",
    "plt.title('s1['+str(i)+\"]-s1_T[\"+str(i)+\"]\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
